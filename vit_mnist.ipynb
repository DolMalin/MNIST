{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(384)])\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ViT IMAGE](assets/vit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "\t\"\"\"Split image into patches and then embed them.\n",
    "\n",
    "\tParams:\n",
    "\t\timg_size: int\n",
    "\n",
    "\t\tpatch_size: int\n",
    "\n",
    "\t\tin_chans: int\n",
    "\t\t\n",
    "\t\tembed_dim: int\n",
    "\n",
    "\tAttributes:\n",
    "\t\tn_patches : int\n",
    "\t\t\tNumber of patches inside of our image\n",
    "\t\t\n",
    "\t\tproj : nn.Conv2d\n",
    "\t\t\tConvolutional layer that does both the splitting into patches and their embedding\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, img_size, patch_size, in_chans=1, embed_dim=384*2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.img_size = img_size\n",
    "\t\tself.patch_size = patch_size\n",
    "\t\tself.n_patches = (img_size // patch_size) ** 2\n",
    "\t\tself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = x.flatten(2) # flatten patches into a single dimension\n",
    "\t\tx = x.transpose(1, 2)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\t\"\"\"Attention mechanism\n",
    "\t\n",
    "\tParams:\n",
    "\t\tdim: int\n",
    "\t\t\tinput and output dimension of token features\n",
    "\t\t\t\n",
    "\t\tn_heads: int\n",
    "\t\t\tnumber of attention heads\n",
    "\t\t\n",
    "\t\tqkv_bias: bool\n",
    "\t\t\tif True, we include bias to query, key and projections\n",
    "\n",
    "\t\tattn_p: float\n",
    "\t\t\tdropout probability applied to the query, key and value tensors\n",
    "\n",
    "\t\tproj_p: float\n",
    "\t\t\tdropout probability applied to the output tensor\n",
    "\n",
    "\tAttributes:\n",
    "\t\tscale: float\n",
    "\t\t\tnormalizing for the doat product\n",
    "\n",
    "\t\tqkv: nn.Linear\n",
    "\t\t\tget a token and make a linear projection for the query, key and value\n",
    "\n",
    "\t\tproj: nn.Linear\n",
    "\t\t\tlinear mapping that takes in the concatenated output of all attention heads and maps it into a new space\n",
    "\t\t\n",
    "\t\tattn_drop, proj_drop: nn.Dropout\n",
    "\t\t\tdropout layers\n",
    "\t\t\"\"\"\n",
    "\tdef __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.dim = dim\n",
    "\t\tself.head_dim = dim // n_heads\n",
    "\t\tself.scale = self.head_dim ** -0.5\n",
    "\n",
    "\t\tself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_p)\n",
    "\t\tself.proj = nn.Linear(dim, dim)\n",
    "\t\tself.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tn_samples, n_tokens, dim = x.shape\n",
    "\n",
    "\t\tif dim != self.dim:\n",
    "\t\t\traise ValueError\n",
    "\t\t\n",
    "\t\tqkv = self.qkv(x)\n",
    "\t\tqkv = qkv.reshape(n_samples, n_tokens, 1, self.n_heads, self.head_dim) # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
    "\t\tqkv = qkv.permute(2, 0, 3, 1, 4) # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\t\tk_t = k.transpose(-2, -1) # (n_samples, n_heads, head_dim, n_patches + 1)\n",
    "\t\tdot_product = (q @ k_t) * self.scale # (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "\t\tattn = dot_product.softmax(dim=-1)\n",
    "\n",
    "\t\tweighted_average = attn @ v\n",
    "\t\tweighted_average = weighted_average = weighted_average.transpose(1, 2) #(n_samples, n_patches + 1, n_heads, head_dim)\n",
    "\t\tweighted_average = weighted_average.flatten(2) # (n_samples, n_patches + 1, dim)\n",
    "\n",
    "\t\tx = self.proj(weighted_average) # (n_samples, n_patches + 1, dim)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\t\"\"\"Multilayer perceptron\n",
    "\t\n",
    "\tParams:\n",
    "\t\tin_features: int\n",
    "\t\t\tnumber of input features\n",
    "\n",
    "\t\thidden_features: int\n",
    "\t\t\tnumber of nodes in the hidden layer\n",
    "\n",
    "\t\tout_features: int\n",
    "\t\t\tnumber of output features\n",
    "\t\t\n",
    "\t\tp: float\n",
    "\t\t\tdropout probability\n",
    "\t\n",
    "\tAttributes:\n",
    "\t\tfc: nn.Linear\n",
    "\t\t\tfirst linear layer\n",
    "\t\t\n",
    "\t\tact: nn.GELU\n",
    "\t\t\tGELU action function\n",
    "\t\t\n",
    "\t\tfc2: nn.Linear\n",
    "\t\t\tsecond linear layer\n",
    "\n",
    "\t\tdrop: nn.Dropout\n",
    "\t\t\tDropout layer\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.fc1 = nn.Linear(in_features, hidden_features)\n",
    "\t\tself.act = nn.GELU()\n",
    "\t\tself.fc2 = nn.Linear(hidden_features, out_features)\n",
    "\t\tself.drop = nn.Dropout(p)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1\n",
    "\t\tx = self.act(x)\n",
    "\t\tx = self.drop(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.drop(x)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\t\"\"\"Transformer Block\n",
    "\t\n",
    "\tParams:\n",
    "\t\tdim: int\n",
    "\t\t\tembedding dimension\n",
    "\n",
    "\t\tn_heads: int\n",
    "\t\t\tnumber of attention heads\n",
    "\t\t\t\n",
    "\t\tmlp_ratio: float\n",
    "\t\t\thidden_dimension size of the MLP module with respect to dim\n",
    "\t\t\n",
    "\t\tqkv_bias: bool\n",
    "\t\t\tif True, include bias to query, key and value projections\n",
    "\n",
    "\t\tp, attn_p: float\n",
    "\t\t\tdropout probability\n",
    "\n",
    "\tAttributes:\n",
    "\t\tnorm1, norm2: LayerNorm\n",
    "\t\t\tnormalization layers\n",
    "\n",
    "\t\tattn: Attention\n",
    "\t\t\tAttention module\n",
    "\n",
    "\t\tmlp: MLP\n",
    "\t\t\tMLP Module\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, dim, n_heads, mlp_ratio=0.4, qkv_bias=True, p=0., attn_p=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\t\tself.attn = Attention(dim, n_heads=n_heads, qkv_bias=qkv_bias, attn_p=attn_p, proj_p=p)\n",
    "\t\tself.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "\n",
    "\t\thidden_features = int(dim * mlp_ratio)\n",
    "\t\tself.mlp = MLP(in_features=dim, hidden_features=hidden_features,out_features=dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = x + self.attn(self.norm1(x))\n",
    "\t\tx = x + self.mlp(self.norm2(x))\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\t\"\"\"Vision Transformer module\n",
    "\t\n",
    "\tParams:\n",
    "\t\timg_size: int\n",
    "\t\t\theight and width of a squared image\n",
    "\t\t\t\n",
    "\t\tpatch_size: int\n",
    "\t\t\theight and width of the patch\n",
    "\t\t\t\n",
    "\t\tin_chans: int\n",
    "\t\t\tnumber of input channel\n",
    "\t\t\t\n",
    "\t\tn_classes: int\n",
    "\t\t\tnumber of classes\n",
    "\t\t\t\n",
    "\t\tembed_dim: int\n",
    "\t\t\tdimension of the token/patch embedding\n",
    "\t\t\t\n",
    "\t\tdepth: int\n",
    "\t\t\tnumber of blocks\n",
    "\t\t\t\n",
    "\t\tn_heads: int\n",
    "\t\t\tnumber of attention heads\n",
    "\n",
    "\t\tmlp_ratio: float\n",
    "\t\t\thidden dimension of the MLP module\n",
    "\n",
    "\t\tqkv_bias: bool\n",
    "\t\t\tif True, include bias to query, key and value projections\n",
    "\n",
    "\t\tp, attn_p: float\n",
    "\t\t\tdropout probability\n",
    "\n",
    "\tAttributes:\n",
    "\t\tpatch_embed: PatchEmbed\n",
    "\t\t\tinstance of PatchEmbed layer\n",
    "\n",
    "\t\tcls_token: nn.Parameter\n",
    "\t\t\tlearnable parameter that is the first token in the sequence\n",
    "\n",
    "\t\tpos_emb: nn.Parameter\n",
    "\t\t\tPositionnal embedding of the cls token + all the patches\n",
    "\n",
    "\t\tpos_drop: nn.Dropout\n",
    "\t\t\tdropout layer\n",
    "\n",
    "\t\tblocks: nn.ModuleList\n",
    "\t\t\tList of Block modules.\n",
    "\n",
    "\t\tnorm: nn.LayerNorm\n",
    "\t\t\tLayer normalization\n",
    "\t\t\"\"\"\n",
    "\tdef __init__(self, img_size=28, patch_size=5, in_chans=1, n_classes=10, embed_dim=56, depth=12, n_heads=3, mlp_ratio=4, qkv_bias=True, p=0., attn_p=0.):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "\t\tself.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\t\tself.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "\t\tself.pos_drop = nn.Dropout(p=p)\n",
    "\t\tself.blocks = nn.ModuleList([\n",
    "\t\t\tBlock(dim=embed_dim, n_heads=n_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, p=p, attn_p=attn_p) \n",
    "\t\t\tfor _ in range(depth)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "\t\tself.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tn_samples = x.shape[0]\n",
    "\t\tx = self.patch_embed(x)\n",
    "\n",
    "\t\tcls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, embed_dim)\n",
    "\t\tx = torch.cat((cls_token, x), dim = 1) # (n_samples, 1 + n_patches, embed_dim)\n",
    "\t\tx = self.pos_drop(x)\n",
    "\n",
    "\t\tfor block in self.blocks:\n",
    "\t\t\tx = block(x)\n",
    "\n",
    "\t\tx = self.norm(x)\n",
    "\n",
    "\t\tcls_token_final = x[:, 0]\n",
    "\t\tx = self.head(cls_token_final)\n",
    "\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VisionTransformer()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "\tsize = len(train_loader.dataset)\n",
    "\tbatches_l = len(train_loader)\n",
    "\tloss = 0\n",
    "\tcorrect = 0\n",
    "\n",
    "\tfor batch_idx, (data, target) in enumerate(train_loader):\n",
    "\t\tpred = model(data)\n",
    "\t\tloss = loss_fn(pred, target)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tif batch_idx % 100 == 0:\n",
    "\t\t\tprint(f'{loss= }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 5777, 1, 3, 18]' is invalid for input of size 31057152",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000021?line=0'>1</a>\u001b[0m train(train_loader, model, loss_fn, optimizer)\n",
      "\u001b[1;32m/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb Cell 18'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000020?line=4'>5</a>\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000020?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000020?line=7'>8</a>\u001b[0m \tpred \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000020?line=8'>9</a>\u001b[0m \tloss \u001b[39m=\u001b[39m loss_fn(pred, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000020?line=10'>11</a>\u001b[0m \toptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb Cell 16'\u001b[0m in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000018?line=74'>75</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_drop(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000018?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000018?line=77'>78</a>\u001b[0m \tx \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000018?line=79'>80</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000018?line=81'>82</a>\u001b[0m cls_token_final \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb Cell 14'\u001b[0m in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000015?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000015?line=40'>41</a>\u001b[0m \tx \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000015?line=41'>42</a>\u001b[0m \tx \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000015?line=43'>44</a>\u001b[0m \t\u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/pdal-mol/work/42AI/miniconda3/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb Cell 10'\u001b[0m in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000012?line=48'>49</a>\u001b[0m \t\u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000012?line=50'>51</a>\u001b[0m qkv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkv(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000012?line=51'>52</a>\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39;49mreshape(n_samples, n_tokens, \u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_heads, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim) \u001b[39m# (n_samples, n_patches + 1, 3, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000012?line=52'>53</a>\u001b[0m qkv \u001b[39m=\u001b[39m qkv\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39m# (3, n_samples, n_heads, n_patches + 1, head_dim)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pdal-mol/work/42AI/MNIST/vit_mnist.ipynb#ch0000012?line=53'>54</a>\u001b[0m q, k, v \u001b[39m=\u001b[39m qkv[\u001b[39m0\u001b[39m], qkv[\u001b[39m1\u001b[39m], qkv[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 5777, 1, 3, 18]' is invalid for input of size 31057152"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd5130da47275f51a83b010c3d8dc9df6d7e608e380d48a8f39d9e300bc08c4f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Hand2Text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
